{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corporate-eleven",
   "metadata": {},
   "source": [
    "# NTU Consumption Forecasting - Neural Networks\n",
    "\n",
    "The following notebook has the objective of tuning a neural network model to forecast real power consumption from SBS with the data provided by ODFM. The inputs of the neural network will be the hour of the day, the day of the week and the month of the year at which each consumption occurs. Additionally, irradiation generated power will also be an input as it has been determined to have high correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thorough-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all needed libraries and sublibraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mathematical-proposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>Month</th>\n",
       "      <th>NEC_P</th>\n",
       "      <th>NEC_Q</th>\n",
       "      <th>CANTEEN_2_P</th>\n",
       "      <th>CANTEEN_2_Q</th>\n",
       "      <th>SPMS_P</th>\n",
       "      <th>SPMS_Q</th>\n",
       "      <th>RTP_P</th>\n",
       "      <th>...</th>\n",
       "      <th>THE_WAVE_P</th>\n",
       "      <th>THE_WAVE_Q</th>\n",
       "      <th>HALL_4_P</th>\n",
       "      <th>HALL_4_Q</th>\n",
       "      <th>EMB_P</th>\n",
       "      <th>EMB_Q</th>\n",
       "      <th>NYA_P</th>\n",
       "      <th>NYA_Q</th>\n",
       "      <th>NYH_P</th>\n",
       "      <th>Irradiation_P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>297.599840</td>\n",
       "      <td>57.360012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2662.0004</td>\n",
       "      <td>1060.99997</td>\n",
       "      <td>421.200002</td>\n",
       "      <td>...</td>\n",
       "      <td>23.899985</td>\n",
       "      <td>-5.200005</td>\n",
       "      <td>111.59994</td>\n",
       "      <td>-32.80001</td>\n",
       "      <td>1017.07703</td>\n",
       "      <td>0.402891</td>\n",
       "      <td>27.700012</td>\n",
       "      <td>4.000014</td>\n",
       "      <td>43.99999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>273.200038</td>\n",
       "      <td>56.640105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2635.0006</td>\n",
       "      <td>1053.00015</td>\n",
       "      <td>416.100030</td>\n",
       "      <td>...</td>\n",
       "      <td>23.700000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>99.80003</td>\n",
       "      <td>-32.09999</td>\n",
       "      <td>1028.20990</td>\n",
       "      <td>8.366951</td>\n",
       "      <td>27.199999</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>45.50000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 02:00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>267.599995</td>\n",
       "      <td>58.759954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2673.0006</td>\n",
       "      <td>1079.00030</td>\n",
       "      <td>445.999890</td>\n",
       "      <td>...</td>\n",
       "      <td>23.400001</td>\n",
       "      <td>-4.900002</td>\n",
       "      <td>105.99997</td>\n",
       "      <td>-34.39999</td>\n",
       "      <td>1041.81469</td>\n",
       "      <td>1.116322</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>3.999958</td>\n",
       "      <td>46.69992</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:00:00</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>266.011430</td>\n",
       "      <td>56.320000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2622.0004</td>\n",
       "      <td>1078.00010</td>\n",
       "      <td>430.099970</td>\n",
       "      <td>...</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>-5.200000</td>\n",
       "      <td>106.59999</td>\n",
       "      <td>-32.30001</td>\n",
       "      <td>1023.37623</td>\n",
       "      <td>24.023428</td>\n",
       "      <td>25.300030</td>\n",
       "      <td>0.900065</td>\n",
       "      <td>42.90001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>268.406032</td>\n",
       "      <td>57.246219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2653.0003</td>\n",
       "      <td>1078.00000</td>\n",
       "      <td>415.600017</td>\n",
       "      <td>...</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>-4.900000</td>\n",
       "      <td>103.99990</td>\n",
       "      <td>-33.50002</td>\n",
       "      <td>1014.89830</td>\n",
       "      <td>3.295560</td>\n",
       "      <td>28.499998</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>47.60005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Hour  Day of Week  Month       NEC_P      NEC_Q  \\\n",
       "TIME                                                                   \n",
       "2020-01-01 00:00:00     0            4      1  297.599840  57.360012   \n",
       "2020-01-01 01:00:00     1            4      1  273.200038  56.640105   \n",
       "2020-01-01 02:00:00     2            4      1  267.599995  58.759954   \n",
       "2020-01-01 03:00:00     3            4      1  266.011430  56.320000   \n",
       "2020-01-01 04:00:00     4            4      1  268.406032  57.246219   \n",
       "\n",
       "                     CANTEEN_2_P  CANTEEN_2_Q     SPMS_P      SPMS_Q  \\\n",
       "TIME                                                                   \n",
       "2020-01-01 00:00:00          0.0          0.0  2662.0004  1060.99997   \n",
       "2020-01-01 01:00:00          0.0          0.0  2635.0006  1053.00015   \n",
       "2020-01-01 02:00:00          0.0          0.0  2673.0006  1079.00030   \n",
       "2020-01-01 03:00:00          0.0          0.0  2622.0004  1078.00010   \n",
       "2020-01-01 04:00:00          0.0          0.0  2653.0003  1078.00000   \n",
       "\n",
       "                          RTP_P  ...  THE_WAVE_P  THE_WAVE_Q   HALL_4_P  \\\n",
       "TIME                             ...                                      \n",
       "2020-01-01 00:00:00  421.200002  ...   23.899985   -5.200005  111.59994   \n",
       "2020-01-01 01:00:00  416.100030  ...   23.700000   -5.000000   99.80003   \n",
       "2020-01-01 02:00:00  445.999890  ...   23.400001   -4.900002  105.99997   \n",
       "2020-01-01 03:00:00  430.099970  ...   23.200000   -5.200000  106.59999   \n",
       "2020-01-01 04:00:00  415.600017  ...   22.800000   -4.900000  103.99990   \n",
       "\n",
       "                     HALL_4_Q       EMB_P      EMB_Q      NYA_P     NYA_Q  \\\n",
       "TIME                                                                        \n",
       "2020-01-01 00:00:00 -32.80001  1017.07703   0.402891  27.700012  4.000014   \n",
       "2020-01-01 01:00:00 -32.09999  1028.20990   8.366951  27.199999  4.600000   \n",
       "2020-01-01 02:00:00 -34.39999  1041.81469   1.116322  27.100000  3.999958   \n",
       "2020-01-01 03:00:00 -32.30001  1023.37623  24.023428  25.300030  0.900065   \n",
       "2020-01-01 04:00:00 -33.50002  1014.89830   3.295560  28.499998  4.000000   \n",
       "\n",
       "                        NYH_P  Irradiation_P  \n",
       "TIME                                          \n",
       "2020-01-01 00:00:00  43.99999            0.0  \n",
       "2020-01-01 01:00:00  45.50000            0.0  \n",
       "2020-01-01 02:00:00  46.69992            0.0  \n",
       "2020-01-01 03:00:00  42.90001            0.0  \n",
       "2020-01-01 04:00:00  47.60005            0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset to be used\n",
    "NTU_Power = pd.read_excel('NNData.xlsx', index_col=0)\n",
    "NTU_Power.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reported-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory that will be used to store the results of the network tuning\n",
    "LOG_DIR3 = f\"{int(time.time())}\"\n",
    "\n",
    "# Choose the variables that will be employed to train and test the neural network\n",
    "# The building chosen is SBS as it has a strong correlation with irradiance (hence this variable is used as predictor)\n",
    "# Inputs for the neural network will be hour, day of the week, month and irradiance\n",
    "# Response variable to be predicted will be SBS_P\n",
    "X = pd.DataFrame(NTU_Power[[\"Hour\",\"Day of Week\", \"Month\", \"Irradiation_P\"]])\n",
    "y = pd.DataFrame(NTU_Power[\"SBS_P\"])\n",
    "\n",
    "# Split the data into trainning and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grave-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model to be built\n",
    "# Add dynamic hyperparameters to allow for tuning the model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hp.Int(\"input_units\", 4, 16, 2), input_shape=X_train.shape[1:], activation='relu'))\n",
    "    \n",
    "    for i in range(hp.Int(\"n_layers\", 1, 5)):\n",
    "        model.add(Dense(hp.Int(f\"HidLayer_{i}_units\", 4, 16, 2), activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(1,))\n",
    "    \n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', [0.001, 0.01, 0.1])), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dramatic-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuner object\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective = \"val_mean_squared_error\",\n",
    "    max_trials = 100,\n",
    "    executions_per_trial = 1,\n",
    "    directory=LOG_DIR3\n",
    ")\n",
    "\n",
    "# Add an early stopper object for neural network convergence\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "# Pickle to store the results of the tuning process\n",
    "with open(f\"tuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tuner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "confirmed-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 01m 20s]\n",
      "val_mean_squared_error: 23189.0234375\n",
      "\n",
      "Best val_mean_squared_error So Far: 16478.451171875\n",
      "Total elapsed time: 02h 48m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Start tuner search\n",
    "tuner.search(\n",
    "    x = X_train, \n",
    "    y=y_train, \n",
    "    epochs = 1000, \n",
    "    batch_size = 64, \n",
    "    validation_split = 0.2,\n",
    "    callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pointed-spring",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_units': 14, 'n_layers': 5, 'HidLayer_0_units': 14, 'learning_rate': 0.01, 'HidLayer_1_units': 8, 'HidLayer_2_units': 10, 'HidLayer_3_units': 12, 'HidLayer_4_units': 12}\n"
     ]
    }
   ],
   "source": [
    "# Obtain the best model and its hyperparameters\n",
    "print(tuner.get_best_hyperparameters()[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naughty-communist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in 1618225418\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_mean_squared_error', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 14\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 8\n",
      "HidLayer_2_units: 10\n",
      "HidLayer_3_units: 12\n",
      "HidLayer_4_units: 12\n",
      "Score: 16478.451171875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 6\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 10\n",
      "HidLayer_2_units: 4\n",
      "HidLayer_3_units: 16\n",
      "Score: 16889.1484375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 10\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 8\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 6\n",
      "HidLayer_2_units: 12\n",
      "HidLayer_3_units: 10\n",
      "HidLayer_4_units: 6\n",
      "Score: 18895.873046875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 3\n",
      "HidLayer_0_units: 8\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 8\n",
      "HidLayer_2_units: 6\n",
      "HidLayer_3_units: 8\n",
      "HidLayer_4_units: 4\n",
      "Score: 19551.224609375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 10\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 14\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 12\n",
      "HidLayer_3_units: 6\n",
      "HidLayer_4_units: 8\n",
      "Score: 21510.494140625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 4\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 16\n",
      "HidLayer_2_units: 14\n",
      "HidLayer_3_units: 16\n",
      "HidLayer_4_units: 6\n",
      "Score: 21560.830078125\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 1\n",
      "HidLayer_0_units: 4\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 12\n",
      "HidLayer_2_units: 14\n",
      "HidLayer_3_units: 8\n",
      "HidLayer_4_units: 4\n",
      "Score: 21806.38671875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 8\n",
      "n_layers: 3\n",
      "HidLayer_0_units: 6\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 8\n",
      "HidLayer_3_units: 12\n",
      "Score: 22015.30859375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 8\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 16\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 10\n",
      "HidLayer_2_units: 8\n",
      "HidLayer_3_units: 4\n",
      "Score: 22437.640625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 1\n",
      "HidLayer_0_units: 14\n",
      "learning_rate: 0.01\n",
      "HidLayer_1_units: 6\n",
      "HidLayer_2_units: 6\n",
      "HidLayer_3_units: 8\n",
      "HidLayer_4_units: 6\n",
      "Score: 23080.203125\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Summary of the tuning process\n",
    "print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-bones",
   "metadata": {},
   "source": [
    "## Observations on parameters to be tuned\n",
    "\n",
    "#### Learning rate\n",
    "All ten best models having learning rate equal to 0.01. This is definitely the best learning rate.\n",
    "\n",
    "#### Number of layers\n",
    "Most of the best performing models have 4 or 5 layers. The problem favours deep structure.\n",
    "\n",
    "#### Input units\n",
    "No strong assumption can e made, but several models seem to have a high number of neurons.\n",
    "\n",
    "#### Hidden layer units\n",
    "No conclusion can be drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-enough",
   "metadata": {},
   "source": [
    "## How does pre-processing (scaling) affect performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unlimited-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale both training and testing input data\n",
    "\n",
    "X_train = preprocessing.scale(X_train)\n",
    "\n",
    "X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mental-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model to be built\n",
    "# Add dynamic hyperparameters to allow for tuning  the model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hp.Int(\"input_units\", 4, 16, 2), input_shape=X_train.shape[1:], activation='relu'))\n",
    "    \n",
    "    for i in range(hp.Int(\"n_layers\", 1, 5)):\n",
    "        model.add(Dense(hp.Int(f\"HidLayer_{i}_units\", 4, 16, 2), activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(1,))\n",
    "    \n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', [0.001, 0.01, 0.1])), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "minus-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory that will be used to store the results of the network tuning\n",
    "LOG_DIR5 = f\"{int(time.time())}\"\n",
    "\n",
    "# Tuner object\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective = \"val_mean_squared_error\",\n",
    "    max_trials = 100,\n",
    "    executions_per_trial = 2,\n",
    "    directory=LOG_DIR5\n",
    ")\n",
    "\n",
    "# Add an early stopper object for neural network convergence\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "# Pickle to store the results of the tuning process\n",
    "with open(f\"tuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tuner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "intellectual-highlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 02m 01s]\n",
      "val_mean_squared_error: 37396.2822265625\n",
      "\n",
      "Best val_mean_squared_error So Far: 14800.3056640625\n",
      "Total elapsed time: 01h 59m 25s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Start tuner search\n",
    "tuner.search(\n",
    "    x = X_train, \n",
    "    y=y_train, \n",
    "    epochs = 2000, \n",
    "    batch_size = 64, \n",
    "    validation_split = 0.2,\n",
    "    callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "human-speaking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_units': 16, 'n_layers': 2, 'HidLayer_0_units': 10, 'learning_rate': 0.1, 'HidLayer_1_units': 10, 'HidLayer_2_units': 6, 'HidLayer_3_units': 14, 'HidLayer_4_units': 14}\n"
     ]
    }
   ],
   "source": [
    "# Obtain the best model and its hyperparameters\n",
    "print(tuner.get_best_hyperparameters()[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "military-arabic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in 1618243967\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_mean_squared_error', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 16\n",
      "n_layers: 2\n",
      "HidLayer_0_units: 10\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 10\n",
      "HidLayer_2_units: 6\n",
      "HidLayer_3_units: 14\n",
      "HidLayer_4_units: 14\n",
      "Score: 14800.3056640625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 3\n",
      "HidLayer_0_units: 16\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 12\n",
      "HidLayer_2_units: 4\n",
      "HidLayer_3_units: 10\n",
      "HidLayer_4_units: 10\n",
      "Score: 15483.068359375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 3\n",
      "HidLayer_0_units: 8\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 10\n",
      "HidLayer_3_units: 4\n",
      "HidLayer_4_units: 8\n",
      "Score: 15627.814453125\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 8\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 14\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 16\n",
      "HidLayer_2_units: 4\n",
      "HidLayer_3_units: 12\n",
      "HidLayer_4_units: 8\n",
      "Score: 15709.255859375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 3\n",
      "HidLayer_0_units: 4\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 12\n",
      "HidLayer_2_units: 6\n",
      "HidLayer_3_units: 12\n",
      "HidLayer_4_units: 14\n",
      "Score: 15741.85888671875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 14\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 16\n",
      "HidLayer_3_units: 14\n",
      "HidLayer_4_units: 16\n",
      "Score: 15784.0146484375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 3\n",
      "HidLayer_0_units: 10\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 12\n",
      "HidLayer_2_units: 10\n",
      "HidLayer_3_units: 6\n",
      "HidLayer_4_units: 4\n",
      "Score: 15880.21337890625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 3\n",
      "HidLayer_0_units: 10\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 8\n",
      "HidLayer_2_units: 16\n",
      "HidLayer_3_units: 4\n",
      "HidLayer_4_units: 8\n",
      "Score: 15937.0478515625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 16\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 8\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 12\n",
      "HidLayer_2_units: 12\n",
      "HidLayer_3_units: 8\n",
      "HidLayer_4_units: 14\n",
      "Score: 16104.212890625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 8\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 14\n",
      "learning_rate: 0.1\n",
      "HidLayer_1_units: 8\n",
      "HidLayer_2_units: 14\n",
      "HidLayer_3_units: 12\n",
      "HidLayer_4_units: 10\n",
      "Score: 16668.13037109375\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Summary of the tuning process\n",
    "print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-garbage",
   "metadata": {},
   "source": [
    "## Changes due to preprocessing\n",
    "Now, the learning rate that clearly leads to the best results is 0.1. This is,thanks to scaling of data we are able to increase the speed at which the model is trained, hence it is a good change.\n",
    "No definitive results can be obtained with respect to the speed of execution, since the number of epochs allowed for this tuning was higher (as it was observed that the previous tuning process did not run for so long).\n",
    "The other trends that seem to be favoured (large number of layers, especially, and large number of neurons) remain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-modification",
   "metadata": {},
   "source": [
    "## Steps taken\n",
    "\n",
    "- From now on, use preprocessed data\n",
    "\n",
    "- Run model with higher number of layers and neurons per layer\n",
    "\n",
    "- Allow for change of learning rate close to 0.1\n",
    "\n",
    "- Allow for change in the parameter beta_1 corresponding to momentum\n",
    "\n",
    "- Allow for change in the parameter beta_1 corresponding to rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "super-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a directory that will be used to store the results of the network tuning\n",
    "LOG_DIR4 = f\"{int(time.time())}\"\n",
    "\n",
    "\n",
    "# Define the neural network model to be built\n",
    "# Add dynamic hyperparameters to allow for tuning of the model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hp.Int(\"input_units\", 12, 16, 2), input_shape=X_train.shape[1:], activation='relu'))\n",
    "    \n",
    "    for i in range(hp.Int(\"n_layers\", 4, 6)):\n",
    "        model.add(Dense(hp.Int(f\"HidLayer_{i}_units\", 12, 16, 2), activation = 'relu'))\n",
    "    \n",
    "    model.add(Dense(1,))\n",
    "    \n",
    "    model.compile(optimizer=Adam(hp.Choice('learning_rate', [0.05, 0.1, 0.2]), hp.Choice('beta_1', [0.8, 0.9, 0.95]), hp.Choice('beta_2', [ 0.95, 0.999])), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "heated-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective = \"val_mean_squared_error\",\n",
    "    max_trials = 200,\n",
    "    executions_per_trial = 2,\n",
    "    directory=LOG_DIR4\n",
    ")\n",
    "\n",
    "# Add an early stopper object for neural network convergence\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "\n",
    "# Pickle to store the results of the tuning process\n",
    "with open(f\"tuner_{int(time.time())}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tuner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "white-creek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 200 Complete [00h 02m 28s]\n",
      "val_mean_squared_error: 16009.86083984375\n",
      "\n",
      "Best val_mean_squared_error So Far: 13977.59619140625\n",
      "Total elapsed time: 05h 32m 55s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Start tuner search\n",
    "tuner.search(\n",
    "    x = X_train, \n",
    "    y=y_train, \n",
    "    epochs = 2000, \n",
    "    batch_size = 64, \n",
    "    validation_split = 0.2,\n",
    "    callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "civic-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in 1618278069\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_mean_squared_error', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 14\n",
      "HidLayer_1_units: 12\n",
      "HidLayer_2_units: 12\n",
      "HidLayer_3_units: 14\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 14\n",
      "HidLayer_5_units: 14\n",
      "Score: 13977.59619140625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 16\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 14\n",
      "HidLayer_3_units: 14\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 14\n",
      "HidLayer_5_units: 12\n",
      "Score: 14035.853515625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 14\n",
      "HidLayer_1_units: 12\n",
      "HidLayer_2_units: 12\n",
      "HidLayer_3_units: 16\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.8\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 14\n",
      "HidLayer_5_units: 14\n",
      "Score: 14084.677734375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 16\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 16\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 14\n",
      "HidLayer_3_units: 16\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 16\n",
      "HidLayer_5_units: 12\n",
      "Score: 14114.59375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 14\n",
      "HidLayer_1_units: 16\n",
      "HidLayer_2_units: 14\n",
      "HidLayer_3_units: 14\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 12\n",
      "HidLayer_5_units: 14\n",
      "Score: 14145.55029296875\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 6\n",
      "HidLayer_0_units: 14\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 14\n",
      "HidLayer_3_units: 16\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 14\n",
      "HidLayer_5_units: 16\n",
      "Score: 14200.50439453125\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 4\n",
      "HidLayer_0_units: 14\n",
      "HidLayer_1_units: 16\n",
      "HidLayer_2_units: 12\n",
      "HidLayer_3_units: 16\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.9\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 14\n",
      "HidLayer_5_units: 14\n",
      "Score: 14237.09130859375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 12\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 14\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 16\n",
      "HidLayer_3_units: 12\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.8\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 14\n",
      "HidLayer_5_units: 14\n",
      "Score: 14257.564453125\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 14\n",
      "HidLayer_1_units: 14\n",
      "HidLayer_2_units: 16\n",
      "HidLayer_3_units: 12\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.95\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 14\n",
      "HidLayer_5_units: 14\n",
      "Score: 14268.06103515625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "input_units: 14\n",
      "n_layers: 5\n",
      "HidLayer_0_units: 12\n",
      "HidLayer_1_units: 16\n",
      "HidLayer_2_units: 12\n",
      "HidLayer_3_units: 12\n",
      "learning_rate: 0.05\n",
      "beta_1: 0.8\n",
      "beta_2: 0.999\n",
      "HidLayer_4_units: 16\n",
      "HidLayer_5_units: 12\n",
      "Score: 14286.443359375\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Summary of the tuning process\n",
    "print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-specific",
   "metadata": {},
   "source": [
    "## Observations on parameters to be tuned\n",
    "\n",
    "#### Learning rate\n",
    "All ten best models having learning rate equal to 0.05. This is definitely the best learning rate.\n",
    "\n",
    "#### Number of layers\n",
    "The most repeated number of layers is 5. It seems to be the best choice.\n",
    "\n",
    "#### Input units\n",
    "Most repeated number is 14, a strong assumption cannot be made.\n",
    "\n",
    "#### Hidden layer units\n",
    "Especially 14, but also 16, seems to be the most repeated numbers.\n",
    "\n",
    "#### beta_1\n",
    "No definitive conclusion can be acquired for its value.\n",
    "\n",
    "#### beta_2\n",
    "All ten best models having beta_2 equal to 0.999. This is definitely the best beta_2 value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-pointer",
   "metadata": {},
   "source": [
    "### Best model found\n",
    "The objective now is to see how far this model can go. The model is:\n",
    "\n",
    "Trial summary\n",
    "Hyperparameters:\n",
    "- input_units: 14\n",
    "- n_layers: 5\n",
    "- HidLayer_0_units: 14\n",
    "- HidLayer_1_units: 12\n",
    "- HidLayer_2_units: 12\n",
    "- HidLayer_3_units: 14\n",
    "- learning_rate: 0.05\n",
    "- beta_1: 0.9\n",
    "- beta_2: 0.999\n",
    "- HidLayer_4_units: 14\n",
    "- HidLayer_5_units: 14\n",
    "- Score: 13977.59619140625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "light-thailand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "88/88 [==============================] - 1s 7ms/step - loss: 968122.2500 - mean_squared_error: 968122.2500 - val_loss: 92624.1797 - val_mean_squared_error: 92624.1797\n",
      "Epoch 2/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 60647.5156 - mean_squared_error: 60647.5156 - val_loss: 51326.8906 - val_mean_squared_error: 51326.8906\n",
      "Epoch 3/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 47480.5938 - mean_squared_error: 47480.5938 - val_loss: 54049.2070 - val_mean_squared_error: 54049.2070\n",
      "Epoch 4/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 43069.6836 - mean_squared_error: 43069.6836 - val_loss: 42883.8164 - val_mean_squared_error: 42883.8164\n",
      "Epoch 5/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 38800.6445 - mean_squared_error: 38800.6445 - val_loss: 36163.2578 - val_mean_squared_error: 36163.2578\n",
      "Epoch 6/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34517.8906 - mean_squared_error: 34517.8906 - val_loss: 41803.2930 - val_mean_squared_error: 41803.2930\n",
      "Epoch 7/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34889.8477 - mean_squared_error: 34889.8477 - val_loss: 32701.5156 - val_mean_squared_error: 32701.5156\n",
      "Epoch 8/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31811.9199 - mean_squared_error: 31811.9199 - val_loss: 27703.4395 - val_mean_squared_error: 27703.4395\n",
      "Epoch 9/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 28816.3965 - mean_squared_error: 28816.3965 - val_loss: 31374.3008 - val_mean_squared_error: 31374.3008\n",
      "Epoch 10/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 37322.5547 - mean_squared_error: 37322.5547 - val_loss: 36170.6992 - val_mean_squared_error: 36170.6992\n",
      "Epoch 11/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33521.5508 - mean_squared_error: 33521.5508 - val_loss: 38739.6914 - val_mean_squared_error: 38739.6914\n",
      "Epoch 12/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 36767.8906 - mean_squared_error: 36767.8906 - val_loss: 29450.5098 - val_mean_squared_error: 29450.5098\n",
      "Epoch 13/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 36180.5742 - mean_squared_error: 36180.5742 - val_loss: 71975.7656 - val_mean_squared_error: 71975.7656\n",
      "Epoch 14/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 34387.8203 - mean_squared_error: 34387.8203 - val_loss: 30069.3652 - val_mean_squared_error: 30069.3652\n",
      "Epoch 15/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 29852.9551 - mean_squared_error: 29852.9551 - val_loss: 40246.4180 - val_mean_squared_error: 40246.4180\n",
      "Epoch 16/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 39164.7852 - mean_squared_error: 39164.7852 - val_loss: 28787.0488 - val_mean_squared_error: 28787.0488\n",
      "Epoch 17/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 32624.7285 - mean_squared_error: 32624.7285 - val_loss: 29858.4590 - val_mean_squared_error: 29858.4590\n",
      "Epoch 18/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34692.2812 - mean_squared_error: 34692.2812 - val_loss: 30419.1152 - val_mean_squared_error: 30419.1152\n",
      "Epoch 19/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 39879.3672 - mean_squared_error: 39879.3672 - val_loss: 48611.8477 - val_mean_squared_error: 48611.8477\n",
      "Epoch 20/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 32047.0117 - mean_squared_error: 32047.0117 - val_loss: 34424.4102 - val_mean_squared_error: 34424.4102\n",
      "Epoch 21/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 28648.5625 - mean_squared_error: 28648.5625 - val_loss: 27720.0996 - val_mean_squared_error: 27720.0996\n",
      "Epoch 22/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36763.7461 - mean_squared_error: 36763.7461 - val_loss: 31334.0488 - val_mean_squared_error: 31334.0488\n",
      "Epoch 23/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 33016.1523 - mean_squared_error: 33016.1523 - val_loss: 30121.3906 - val_mean_squared_error: 30121.3906\n",
      "Epoch 24/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33409.8750 - mean_squared_error: 33409.8750 - val_loss: 47887.2812 - val_mean_squared_error: 47887.2812\n",
      "Epoch 25/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30317.3418 - mean_squared_error: 30317.3418 - val_loss: 43677.8750 - val_mean_squared_error: 43677.8750\n",
      "Epoch 26/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30266.7676 - mean_squared_error: 30266.7676 - val_loss: 33376.8633 - val_mean_squared_error: 33376.8633\n",
      "Epoch 27/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 33297.2188 - mean_squared_error: 33297.2188 - val_loss: 28281.2227 - val_mean_squared_error: 28281.2227\n",
      "Epoch 28/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 38653.6055 - mean_squared_error: 38653.6055 - val_loss: 49917.8477 - val_mean_squared_error: 49917.8477\n",
      "Epoch 29/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 36700.5938 - mean_squared_error: 36700.5938 - val_loss: 31531.1523 - val_mean_squared_error: 31531.1523\n",
      "Epoch 30/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 28465.4023 - mean_squared_error: 28465.4023 - val_loss: 31239.0332 - val_mean_squared_error: 31239.0332\n",
      "Epoch 31/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 30586.5859 - mean_squared_error: 30586.5859 - val_loss: 35354.3125 - val_mean_squared_error: 35354.3125\n",
      "Epoch 32/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 28798.6426 - mean_squared_error: 28798.6426 - val_loss: 39307.8359 - val_mean_squared_error: 39307.8359\n",
      "Epoch 33/5000\n",
      "88/88 [==============================] - 1s 6ms/step - loss: 29415.2422 - mean_squared_error: 29415.2422 - val_loss: 26901.9238 - val_mean_squared_error: 26901.9238\n",
      "Epoch 34/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 30341.5742 - mean_squared_error: 30341.5742 - val_loss: 29186.4902 - val_mean_squared_error: 29186.4902\n",
      "Epoch 35/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 29997.2695 - mean_squared_error: 29997.2695 - val_loss: 40937.2969 - val_mean_squared_error: 40937.2969\n",
      "Epoch 36/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 32771.8867 - mean_squared_error: 32771.8867 - val_loss: 42661.8164 - val_mean_squared_error: 42661.8164\n",
      "Epoch 37/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 34389.2578 - mean_squared_error: 34389.2578 - val_loss: 33457.8047 - val_mean_squared_error: 33457.8047\n",
      "Epoch 38/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 27684.1250 - mean_squared_error: 27684.1250 - val_loss: 26974.3750 - val_mean_squared_error: 26974.3750\n",
      "Epoch 39/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 31701.4570 - mean_squared_error: 31701.4570 - val_loss: 43929.6328 - val_mean_squared_error: 43929.6328\n",
      "Epoch 40/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 28970.9980 - mean_squared_error: 28970.9980 - val_loss: 30826.3320 - val_mean_squared_error: 30826.3320\n",
      "Epoch 41/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30509.0488 - mean_squared_error: 30509.0488 - val_loss: 25873.0098 - val_mean_squared_error: 25873.0098\n",
      "Epoch 42/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 28297.0469 - mean_squared_error: 28297.0469 - val_loss: 31787.4023 - val_mean_squared_error: 31787.4023\n",
      "Epoch 43/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 30476.2988 - mean_squared_error: 30476.2988 - val_loss: 35267.4102 - val_mean_squared_error: 35267.4102\n",
      "Epoch 44/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 31172.4199 - mean_squared_error: 31172.4199 - val_loss: 37474.0664 - val_mean_squared_error: 37474.0664\n",
      "Epoch 45/5000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 27899.5859 - mean_squared_error: 27899.5859 - val_loss: 24501.6836 - val_mean_squared_error: 24501.6836\n",
      "Epoch 46/5000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 25802.5547 - mean_squared_error: 25802.5547 - val_loss: 29677.6211 - val_mean_squared_error: 29677.6211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 28653.0957 - mean_squared_error: 28653.0957 - val_loss: 30615.1582 - val_mean_squared_error: 30615.1582\n",
      "Epoch 48/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 30263.0195 - mean_squared_error: 30263.0195 - val_loss: 26952.3633 - val_mean_squared_error: 26952.3633\n",
      "Epoch 49/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 25517.4609 - mean_squared_error: 25517.4609 - val_loss: 40450.6445 - val_mean_squared_error: 40450.6445\n",
      "Epoch 50/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 31908.7188 - mean_squared_error: 31908.7188 - val_loss: 30656.1816 - val_mean_squared_error: 30656.1816\n",
      "Epoch 51/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 30798.6191 - mean_squared_error: 30798.6191 - val_loss: 37170.2656 - val_mean_squared_error: 37170.2656\n",
      "Epoch 52/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 25692.1660 - mean_squared_error: 25692.1660 - val_loss: 27196.1133 - val_mean_squared_error: 27196.1133\n",
      "Epoch 53/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 25476.2207 - mean_squared_error: 25476.2207 - val_loss: 26701.9004 - val_mean_squared_error: 26701.9004\n",
      "Epoch 54/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 27142.6543 - mean_squared_error: 27142.6543 - val_loss: 32325.3535 - val_mean_squared_error: 32325.3535\n",
      "Epoch 55/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 25962.6992 - mean_squared_error: 25962.6992 - val_loss: 25899.9609 - val_mean_squared_error: 25899.9609\n",
      "Epoch 56/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 27504.6797 - mean_squared_error: 27504.6797 - val_loss: 26135.6328 - val_mean_squared_error: 26135.6328\n",
      "Epoch 57/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 27216.3438 - mean_squared_error: 27216.3438 - val_loss: 25977.1348 - val_mean_squared_error: 25977.1348\n",
      "Epoch 58/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 29510.6348 - mean_squared_error: 29510.6348 - val_loss: 48912.3711 - val_mean_squared_error: 48912.3711\n",
      "Epoch 59/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 25803.6387 - mean_squared_error: 25803.6387 - val_loss: 25274.9297 - val_mean_squared_error: 25274.9297\n",
      "Epoch 60/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 22870.1094 - mean_squared_error: 22870.1094 - val_loss: 23903.3887 - val_mean_squared_error: 23903.3887\n",
      "Epoch 61/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 24719.4980 - mean_squared_error: 24719.4980 - val_loss: 22947.2910 - val_mean_squared_error: 22947.2910\n",
      "Epoch 62/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 25195.4004 - mean_squared_error: 25195.4004 - val_loss: 22200.7168 - val_mean_squared_error: 22200.7168\n",
      "Epoch 63/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 20737.9746 - mean_squared_error: 20737.9746 - val_loss: 19605.1348 - val_mean_squared_error: 19605.1348\n",
      "Epoch 64/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 20430.2773 - mean_squared_error: 20430.2773 - val_loss: 19469.6074 - val_mean_squared_error: 19469.6074\n",
      "Epoch 65/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 19975.3984 - mean_squared_error: 19975.3984 - val_loss: 17748.7227 - val_mean_squared_error: 17748.7227\n",
      "Epoch 66/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 19429.9629 - mean_squared_error: 19429.9629 - val_loss: 19277.4375 - val_mean_squared_error: 19277.4375\n",
      "Epoch 67/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 21341.5762 - mean_squared_error: 21341.5762 - val_loss: 28168.7207 - val_mean_squared_error: 28168.7207\n",
      "Epoch 68/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 18127.5176 - mean_squared_error: 18127.5176 - val_loss: 18077.9297 - val_mean_squared_error: 18077.9297\n",
      "Epoch 69/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 18686.9336 - mean_squared_error: 18686.9336 - val_loss: 18276.2441 - val_mean_squared_error: 18276.2441\n",
      "Epoch 70/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 18723.3672 - mean_squared_error: 18723.3672 - val_loss: 21774.5195 - val_mean_squared_error: 21774.5195\n",
      "Epoch 71/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 18470.4219 - mean_squared_error: 18470.4219 - val_loss: 18354.4551 - val_mean_squared_error: 18354.4551\n",
      "Epoch 72/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16838.7129 - mean_squared_error: 16838.7129 - val_loss: 19654.0430 - val_mean_squared_error: 19654.0430\n",
      "Epoch 73/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 18001.9121 - mean_squared_error: 18001.9121 - val_loss: 24665.8301 - val_mean_squared_error: 24665.8301\n",
      "Epoch 74/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17176.2129 - mean_squared_error: 17176.2129 - val_loss: 31516.5371 - val_mean_squared_error: 31516.5371\n",
      "Epoch 75/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 20060.8555 - mean_squared_error: 20060.8555 - val_loss: 17141.1621 - val_mean_squared_error: 17141.1621\n",
      "Epoch 76/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17593.1348 - mean_squared_error: 17593.1348 - val_loss: 22805.7520 - val_mean_squared_error: 22805.7520\n",
      "Epoch 77/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17620.3867 - mean_squared_error: 17620.3867 - val_loss: 17814.1328 - val_mean_squared_error: 17814.1328\n",
      "Epoch 78/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16963.6621 - mean_squared_error: 16963.6621 - val_loss: 18518.2441 - val_mean_squared_error: 18518.2441\n",
      "Epoch 79/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16631.4102 - mean_squared_error: 16631.4102 - val_loss: 16831.4805 - val_mean_squared_error: 16831.4805\n",
      "Epoch 80/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 17420.5039 - mean_squared_error: 17420.5039 - val_loss: 23632.3750 - val_mean_squared_error: 23632.3750\n",
      "Epoch 81/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 17794.9531 - mean_squared_error: 17794.9531 - val_loss: 15372.1035 - val_mean_squared_error: 15372.1035\n",
      "Epoch 82/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17791.5176 - mean_squared_error: 17791.5176 - val_loss: 17208.8828 - val_mean_squared_error: 17208.8828\n",
      "Epoch 83/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16739.3262 - mean_squared_error: 16739.3262 - val_loss: 18434.0664 - val_mean_squared_error: 18434.0664\n",
      "Epoch 84/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 17213.0469 - mean_squared_error: 17213.0469 - val_loss: 16331.4326 - val_mean_squared_error: 16331.4326\n",
      "Epoch 85/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 17645.3965 - mean_squared_error: 17645.3965 - val_loss: 21456.7617 - val_mean_squared_error: 21456.7617\n",
      "Epoch 86/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16553.6387 - mean_squared_error: 16553.6387 - val_loss: 19616.7109 - val_mean_squared_error: 19616.7109\n",
      "Epoch 87/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16611.2539 - mean_squared_error: 16611.2539 - val_loss: 16058.2500 - val_mean_squared_error: 16058.2500\n",
      "Epoch 88/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 18553.1406 - mean_squared_error: 18553.1406 - val_loss: 17108.3691 - val_mean_squared_error: 17108.3691\n",
      "Epoch 89/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 18154.3691 - mean_squared_error: 18154.3691 - val_loss: 20514.3242 - val_mean_squared_error: 20514.3242\n",
      "Epoch 90/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16856.1504 - mean_squared_error: 16856.1504 - val_loss: 20321.0410 - val_mean_squared_error: 20321.0410\n",
      "Epoch 91/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16631.8633 - mean_squared_error: 16631.8633 - val_loss: 20955.0449 - val_mean_squared_error: 20955.0449\n",
      "Epoch 92/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 17377.6953 - mean_squared_error: 17377.6953 - val_loss: 16482.9199 - val_mean_squared_error: 16482.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/5000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 17202.0605 - mean_squared_error: 17202.0605 - val_loss: 15640.4268 - val_mean_squared_error: 15640.4268\n",
      "Epoch 94/5000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 17092.9219 - mean_squared_error: 17092.9219 - val_loss: 16808.4688 - val_mean_squared_error: 16808.4688\n",
      "Epoch 95/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16050.5459 - mean_squared_error: 16050.5459 - val_loss: 19799.9082 - val_mean_squared_error: 19799.9082\n",
      "Epoch 96/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16535.7148 - mean_squared_error: 16535.7148 - val_loss: 15823.7217 - val_mean_squared_error: 15823.7217\n",
      "Epoch 97/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16498.9336 - mean_squared_error: 16498.9336 - val_loss: 17892.0059 - val_mean_squared_error: 17892.0059\n",
      "Epoch 98/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17806.8535 - mean_squared_error: 17806.8535 - val_loss: 23185.2012 - val_mean_squared_error: 23185.2012\n",
      "Epoch 99/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16392.3242 - mean_squared_error: 16392.3242 - val_loss: 16293.4209 - val_mean_squared_error: 16293.4209\n",
      "Epoch 100/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 18080.3730 - mean_squared_error: 18080.3730 - val_loss: 18298.7891 - val_mean_squared_error: 18298.7891\n",
      "Epoch 101/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15670.5586 - mean_squared_error: 15670.5586 - val_loss: 16138.6758 - val_mean_squared_error: 16138.6758\n",
      "Epoch 102/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 18579.5371 - mean_squared_error: 18579.5371 - val_loss: 17591.1719 - val_mean_squared_error: 17591.1719\n",
      "Epoch 103/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17042.9238 - mean_squared_error: 17042.9238 - val_loss: 16006.9512 - val_mean_squared_error: 16006.9512\n",
      "Epoch 104/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15809.0732 - mean_squared_error: 15809.0732 - val_loss: 16961.7969 - val_mean_squared_error: 16961.7969\n",
      "Epoch 105/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 16258.8408 - mean_squared_error: 16258.8408 - val_loss: 16054.8633 - val_mean_squared_error: 16054.8633\n",
      "Epoch 106/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16424.2715 - mean_squared_error: 16424.2715 - val_loss: 16169.2529 - val_mean_squared_error: 16169.2529\n",
      "Epoch 107/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15791.4336 - mean_squared_error: 15791.4336 - val_loss: 21158.4258 - val_mean_squared_error: 21158.4258\n",
      "Epoch 108/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15777.1738 - mean_squared_error: 15777.1738 - val_loss: 15652.5410 - val_mean_squared_error: 15652.5410\n",
      "Epoch 109/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15127.0518 - mean_squared_error: 15127.0518 - val_loss: 15382.7627 - val_mean_squared_error: 15382.7627\n",
      "Epoch 110/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 19653.3828 - mean_squared_error: 19653.3828 - val_loss: 15285.8232 - val_mean_squared_error: 15285.8232\n",
      "Epoch 111/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15457.8896 - mean_squared_error: 15457.8896 - val_loss: 15014.3428 - val_mean_squared_error: 15014.3428\n",
      "Epoch 112/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16693.8379 - mean_squared_error: 16693.8379 - val_loss: 20454.9961 - val_mean_squared_error: 20454.9961\n",
      "Epoch 113/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15929.0703 - mean_squared_error: 15929.0703 - val_loss: 18167.8438 - val_mean_squared_error: 18167.8438\n",
      "Epoch 114/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16350.7773 - mean_squared_error: 16350.7773 - val_loss: 16361.8320 - val_mean_squared_error: 16361.8320\n",
      "Epoch 115/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 15578.7070 - mean_squared_error: 15578.7070 - val_loss: 24472.8789 - val_mean_squared_error: 24472.8789\n",
      "Epoch 116/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17277.2012 - mean_squared_error: 17277.2012 - val_loss: 19159.6934 - val_mean_squared_error: 19159.6934\n",
      "Epoch 117/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16269.0449 - mean_squared_error: 16269.0449 - val_loss: 14420.3652 - val_mean_squared_error: 14420.3652\n",
      "Epoch 118/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 18432.9902 - mean_squared_error: 18432.9902 - val_loss: 17698.1113 - val_mean_squared_error: 17698.1113\n",
      "Epoch 119/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16429.5898 - mean_squared_error: 16429.5898 - val_loss: 20944.0508 - val_mean_squared_error: 20944.0508\n",
      "Epoch 120/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15108.1797 - mean_squared_error: 15108.1797 - val_loss: 15496.5518 - val_mean_squared_error: 15496.5518\n",
      "Epoch 121/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15235.6465 - mean_squared_error: 15235.6465 - val_loss: 14800.1836 - val_mean_squared_error: 14800.1836\n",
      "Epoch 122/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15279.6973 - mean_squared_error: 15279.6973 - val_loss: 17026.2578 - val_mean_squared_error: 17026.2578\n",
      "Epoch 123/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15309.5234 - mean_squared_error: 15309.5234 - val_loss: 14973.9160 - val_mean_squared_error: 14973.9160\n",
      "Epoch 124/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15581.7715 - mean_squared_error: 15581.7715 - val_loss: 17091.3535 - val_mean_squared_error: 17091.3535\n",
      "Epoch 125/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15063.6211 - mean_squared_error: 15063.6211 - val_loss: 18336.2988 - val_mean_squared_error: 18336.2988\n",
      "Epoch 126/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 17620.4180 - mean_squared_error: 17620.4180 - val_loss: 28500.2129 - val_mean_squared_error: 28500.2129\n",
      "Epoch 127/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16338.5586 - mean_squared_error: 16338.5586 - val_loss: 16583.7031 - val_mean_squared_error: 16583.7031\n",
      "Epoch 128/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15511.7412 - mean_squared_error: 15511.7412 - val_loss: 24155.2695 - val_mean_squared_error: 24155.2695\n",
      "Epoch 129/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15848.5635 - mean_squared_error: 15848.5635 - val_loss: 17200.6699 - val_mean_squared_error: 17200.6699\n",
      "Epoch 130/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16981.3789 - mean_squared_error: 16981.3789 - val_loss: 21809.2910 - val_mean_squared_error: 21809.2910\n",
      "Epoch 131/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16371.6074 - mean_squared_error: 16371.6074 - val_loss: 17022.1309 - val_mean_squared_error: 17022.1309\n",
      "Epoch 132/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 17221.5879 - mean_squared_error: 17221.5879 - val_loss: 16260.6289 - val_mean_squared_error: 16260.6289\n",
      "Epoch 133/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16603.7773 - mean_squared_error: 16603.7773 - val_loss: 14868.3584 - val_mean_squared_error: 14868.3584\n",
      "Epoch 134/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16004.3066 - mean_squared_error: 16004.3066 - val_loss: 14886.3447 - val_mean_squared_error: 14886.3447\n",
      "Epoch 135/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15725.7129 - mean_squared_error: 15725.7129 - val_loss: 14815.7412 - val_mean_squared_error: 14815.7412\n",
      "Epoch 136/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 14914.0195 - mean_squared_error: 14914.0195 - val_loss: 14605.2646 - val_mean_squared_error: 14605.2646\n",
      "Epoch 137/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15612.0674 - mean_squared_error: 15612.0674 - val_loss: 14846.4307 - val_mean_squared_error: 14846.4326\n",
      "Epoch 138/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 14703.0098 - mean_squared_error: 14703.0098 - val_loss: 17317.0020 - val_mean_squared_error: 17317.0020\n",
      "Epoch 139/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 17009.2656 - mean_squared_error: 17009.2656 - val_loss: 14937.4951 - val_mean_squared_error: 14937.4951\n",
      "Epoch 140/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 14892.2627 - mean_squared_error: 14892.2627 - val_loss: 14165.4590 - val_mean_squared_error: 14165.4590\n",
      "Epoch 141/5000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 15788.3223 - mean_squared_error: 15788.3223 - val_loss: 15276.2490 - val_mean_squared_error: 15276.2490\n",
      "Epoch 142/5000\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 15163.0020 - mean_squared_error: 15163.0020 - val_loss: 15390.2861 - val_mean_squared_error: 15390.2861\n",
      "Epoch 143/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16663.0605 - mean_squared_error: 16663.0605 - val_loss: 14373.5020 - val_mean_squared_error: 14373.5020\n",
      "Epoch 144/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15688.7871 - mean_squared_error: 15688.7871 - val_loss: 15129.7393 - val_mean_squared_error: 15129.7393\n",
      "Epoch 145/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16811.5020 - mean_squared_error: 16811.5020 - val_loss: 18050.7305 - val_mean_squared_error: 18050.7305\n",
      "Epoch 146/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15103.8467 - mean_squared_error: 15103.8467 - val_loss: 13942.5879 - val_mean_squared_error: 13942.5879\n",
      "Epoch 147/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 17582.8145 - mean_squared_error: 17582.8145 - val_loss: 14934.1182 - val_mean_squared_error: 14934.1182\n",
      "Epoch 148/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16503.1035 - mean_squared_error: 16503.1035 - val_loss: 14854.2588 - val_mean_squared_error: 14854.2588\n",
      "Epoch 149/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15164.8857 - mean_squared_error: 15164.8857 - val_loss: 14452.7705 - val_mean_squared_error: 14452.7705\n",
      "Epoch 150/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16383.7178 - mean_squared_error: 16383.7178 - val_loss: 23090.7832 - val_mean_squared_error: 23090.7832\n",
      "Epoch 151/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16274.2783 - mean_squared_error: 16274.2783 - val_loss: 14556.7939 - val_mean_squared_error: 14556.7939\n",
      "Epoch 152/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15417.3594 - mean_squared_error: 15417.3594 - val_loss: 14682.6475 - val_mean_squared_error: 14682.6475\n",
      "Epoch 153/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 14710.9580 - mean_squared_error: 14710.9580 - val_loss: 15115.5576 - val_mean_squared_error: 15115.5576\n",
      "Epoch 154/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 14663.5869 - mean_squared_error: 14663.5869 - val_loss: 14516.4180 - val_mean_squared_error: 14516.4180\n",
      "Epoch 155/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 14683.5947 - mean_squared_error: 14683.5947 - val_loss: 15325.2334 - val_mean_squared_error: 15325.2334\n",
      "Epoch 156/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15898.9121 - mean_squared_error: 15898.9121 - val_loss: 14473.8535 - val_mean_squared_error: 14473.8535\n",
      "Epoch 157/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15851.8896 - mean_squared_error: 15851.8896 - val_loss: 14673.7383 - val_mean_squared_error: 14673.7383\n",
      "Epoch 158/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16798.1816 - mean_squared_error: 16798.1816 - val_loss: 16817.3125 - val_mean_squared_error: 16817.3125\n",
      "Epoch 159/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15703.7656 - mean_squared_error: 15703.7656 - val_loss: 17220.1836 - val_mean_squared_error: 17220.1836\n",
      "Epoch 160/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16289.1816 - mean_squared_error: 16289.1816 - val_loss: 14524.5947 - val_mean_squared_error: 14524.5947\n",
      "Epoch 161/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16602.7930 - mean_squared_error: 16602.7930 - val_loss: 14792.2295 - val_mean_squared_error: 14792.2295\n",
      "Epoch 162/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15921.9424 - mean_squared_error: 15921.9424 - val_loss: 14978.1768 - val_mean_squared_error: 14978.1768\n",
      "Epoch 163/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 19275.0723 - mean_squared_error: 19275.0723 - val_loss: 23268.5293 - val_mean_squared_error: 23268.5293\n",
      "Epoch 164/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15799.1904 - mean_squared_error: 15799.1904 - val_loss: 16952.4883 - val_mean_squared_error: 16952.4883\n",
      "Epoch 165/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15085.4082 - mean_squared_error: 15085.4082 - val_loss: 15029.7412 - val_mean_squared_error: 15029.7412\n",
      "Epoch 166/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15713.1357 - mean_squared_error: 15713.1357 - val_loss: 14731.8564 - val_mean_squared_error: 14731.8564\n",
      "Epoch 167/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15793.5625 - mean_squared_error: 15793.5625 - val_loss: 14665.3389 - val_mean_squared_error: 14665.3389\n",
      "Epoch 168/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 14720.4043 - mean_squared_error: 14720.4043 - val_loss: 16334.6787 - val_mean_squared_error: 16334.6787\n",
      "Epoch 169/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15161.7559 - mean_squared_error: 15161.7559 - val_loss: 15615.2734 - val_mean_squared_error: 15615.2734\n",
      "Epoch 170/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16631.1660 - mean_squared_error: 16631.1660 - val_loss: 16242.1748 - val_mean_squared_error: 16242.1748\n",
      "Epoch 171/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 14947.6963 - mean_squared_error: 14947.6963 - val_loss: 16516.4082 - val_mean_squared_error: 16516.4082\n",
      "Epoch 172/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16061.3057 - mean_squared_error: 16061.3057 - val_loss: 15668.9248 - val_mean_squared_error: 15668.9248\n",
      "Epoch 173/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 19320.1836 - mean_squared_error: 19320.1836 - val_loss: 16207.0244 - val_mean_squared_error: 16207.0244\n",
      "Epoch 174/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15649.0059 - mean_squared_error: 15649.0059 - val_loss: 15575.3301 - val_mean_squared_error: 15575.3301\n",
      "Epoch 175/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 14632.0195 - mean_squared_error: 14632.0195 - val_loss: 15260.5410 - val_mean_squared_error: 15260.5410\n",
      "Epoch 176/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 14788.4844 - mean_squared_error: 14788.4844 - val_loss: 14567.6270 - val_mean_squared_error: 14567.6270\n",
      "Epoch 177/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16327.3613 - mean_squared_error: 16327.3613 - val_loss: 15537.3418 - val_mean_squared_error: 15537.3418\n",
      "Epoch 178/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15405.0088 - mean_squared_error: 15405.0088 - val_loss: 16080.6953 - val_mean_squared_error: 16080.6953\n",
      "Epoch 179/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 16775.1230 - mean_squared_error: 16775.1230 - val_loss: 15800.7129 - val_mean_squared_error: 15800.7129\n",
      "Epoch 180/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15407.8262 - mean_squared_error: 15407.8262 - val_loss: 14328.2314 - val_mean_squared_error: 14328.2314\n",
      "Epoch 181/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15896.9561 - mean_squared_error: 15896.9561 - val_loss: 15046.7734 - val_mean_squared_error: 15046.7734\n",
      "Epoch 182/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 16295.2129 - mean_squared_error: 16295.2129 - val_loss: 17620.6621 - val_mean_squared_error: 17620.6621\n",
      "Epoch 183/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 0s 3ms/step - loss: 15000.4229 - mean_squared_error: 15000.4229 - val_loss: 15305.8184 - val_mean_squared_error: 15305.8184\n",
      "Epoch 184/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15016.5967 - mean_squared_error: 15016.5967 - val_loss: 14541.2998 - val_mean_squared_error: 14541.2998\n",
      "Epoch 185/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15239.2588 - mean_squared_error: 15239.2588 - val_loss: 16917.6582 - val_mean_squared_error: 16917.6582\n",
      "Epoch 186/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15030.8584 - mean_squared_error: 15030.8584 - val_loss: 14442.5225 - val_mean_squared_error: 14442.5225\n",
      "Epoch 187/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 17630.2324 - mean_squared_error: 17630.2324 - val_loss: 19088.5293 - val_mean_squared_error: 19088.5293\n",
      "Epoch 188/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15340.9287 - mean_squared_error: 15340.9287 - val_loss: 15354.5488 - val_mean_squared_error: 15354.5488\n",
      "Epoch 189/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 14983.0176 - mean_squared_error: 14983.0176 - val_loss: 15405.5928 - val_mean_squared_error: 15405.5928\n",
      "Epoch 190/5000\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 15611.1514 - mean_squared_error: 15611.1514 - val_loss: 17270.9004 - val_mean_squared_error: 17270.9004\n",
      "Epoch 191/5000\n",
      "88/88 [==============================] - 0s 3ms/step - loss: 15332.4521 - mean_squared_error: 15332.4521 - val_loss: 19723.0234 - val_mean_squared_error: 19723.0234\n",
      "Epoch 192/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 16183.4453 - mean_squared_error: 16183.4453 - val_loss: 15119.0684 - val_mean_squared_error: 15119.0684\n",
      "Epoch 193/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 15962.6553 - mean_squared_error: 15962.6553 - val_loss: 15978.4697 - val_mean_squared_error: 15978.4697\n",
      "Epoch 194/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 14982.1855 - mean_squared_error: 14982.1855 - val_loss: 20123.1230 - val_mean_squared_error: 20123.1230\n",
      "Epoch 195/5000\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 15285.3770 - mean_squared_error: 15285.3770 - val_loss: 14761.4990 - val_mean_squared_error: 14761.4990\n",
      "Epoch 196/5000\n",
      "88/88 [==============================] - 0s 5ms/step - loss: 14807.0479 - mean_squared_error: 14807.0479 - val_loss: 14824.4336 - val_mean_squared_error: 14824.4336\n",
      "Epoch 00196: early stopping\n",
      "The R2 score on the Train set is:\t0.843\n",
      "The R2 score on the Test set is:\t0.844\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " def build_model1():   \n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(14, input_shape=X_train.shape[1:], activation='relu'))\n",
    "    \n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1,))\n",
    "    # beta_1 and beta_2 parameters are not specified as the best model values on the options tested are the default ones\n",
    "    model.compile(Adam(lr=0.05), loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = build_model1() \n",
    "\n",
    "# Pass several parameters to 'EarlyStopping' function and assigns it to 'earlystopper'\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, mode='auto')\n",
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fits model over 2000 iterations with 'earlystopper' callback, and assigns it to history\n",
    "history = model.fit(X_train, y_train, batch_size = 64, epochs = 5000, validation_split = 0.2,shuffle = True, callbacks = [earlystopper])\n",
    "\n",
    "# Plots 'history'\n",
    "history_dict=history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values=history_dict['val_loss']\n",
    "plt.plot(loss_values,'b',label='training loss')\n",
    "plt.plot(val_loss_values,'r',label='training loss val')\n",
    "\n",
    "# Runs model with its current weights on the training and testing data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculates and prints r2 score of training and testing data\n",
    "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred)))\n",
    "print(\"The R2 score on the Test set is:\\t{:0.3f}\".format(r2_score(y_test, y_test_pred))) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-pillow",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The trained model achieves a strong correlation on both the training and testing datasets, with no signs of overfitting. The tuning of the model is satisfactory. However, it can be tuned further:\n",
    "- Try adding more layers or neurons to the model (increase complexity)\n",
    "- Try different values of the learning rate\n",
    "- Try implementing learning rate decay, for more careful convergence near the global minima\n",
    "- Try different activation functions\n",
    "- Try different techniques such as batch normalization or dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
